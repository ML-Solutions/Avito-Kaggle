{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "E:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "import gensim\n",
    "import re\n",
    "from tqdm import tqdm,tqdm_notebook,tqdm_pandas\n",
    "import gc\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, Imputer\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y, y_pred):\n",
    "    return keras.backend.sqrt(keras.backend.mean(keras.backend.square(y_pred - y)))*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "        `float64` type to `float32`\n",
    "        `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'E:/Kaggle/Avito/'\n",
    "\n",
    "num_words = 100000 #None to deactivate\n",
    "max_len = 150\n",
    "seed = 32\n",
    "FOLDS = 5\n",
    "min_class_cat = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_PATH+'train.csv')\n",
    "test = pd.read_csv(DATA_PATH+'test.csv')\n",
    "guess_image = pd.read_csv(DATA_PATH+'image_guess.csv')\n",
    "price_guess  = pd.read_csv(DATA_PATH+'price_guess.csv')\n",
    "param_guess  = pd.read_csv(DATA_PATH+'param_guess.csv')\n",
    "param2_guess  = pd.read_csv(DATA_PATH+'param2_guess.csv')\n",
    "\n",
    "images0 = pd.read_csv(DATA_PATH+'image0_features.csv')\n",
    "images1 = pd.read_csv(DATA_PATH+'image1_features.csv')\n",
    "images2 = pd.read_csv(DATA_PATH+'image2_features.csv')\n",
    "images3 = pd.read_csv(DATA_PATH+'image3_features.csv')\n",
    "images4 = pd.read_csv(DATA_PATH+'image4_features.csv')\n",
    "imagestest = pd.read_csv(DATA_PATH+'imagetest_features.csv')\n",
    "imagesdata = pd.concat([images0,images1,images2,images3,images4,imagestest],axis=0)\n",
    "del images0, images1, images2, images3, images4, imagestest\n",
    "\n",
    "images_pred0 = pd.read_csv(DATA_PATH+'Image_preds0.csv')\n",
    "images_pred1 = pd.read_csv(DATA_PATH+'Image_preds1.csv')\n",
    "images_pred2 = pd.read_csv(DATA_PATH+'Image_preds2.csv')\n",
    "images_pred3 = pd.read_csv(DATA_PATH+'Image_preds3.csv')\n",
    "images_pred4 = pd.read_csv(DATA_PATH+'Image_preds4.csv')\n",
    "imagestest_pred = pd.read_csv(DATA_PATH+'Image_predstest.csv')\n",
    "imagesdata_pred = pd.concat([images_pred0,images_pred1,images_pred2,images_pred3,images_pred4,imagestest_pred],axis=0)\n",
    "del images_pred0, images_pred1, images_pred2, images_pred3, images_pred4, imagestest_pred\n",
    "\n",
    "user_agg = pd.read_csv(DATA_PATH+'user_aggregated_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(FOLDS,random_state=seed,shuffle=True)\n",
    "Fold = 0\n",
    "for train_index, test_index in kf.split(train):\n",
    "    train.loc[test_index,'Fold']=Fold\n",
    "    Fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data = pd.concat([train,test],axis=0,ignore_index=True)\n",
    "complete_data = pd.merge(complete_data,guess_image,how='left')\n",
    "complete_data = pd.merge(complete_data,price_guess,how='left')\n",
    "complete_data = pd.merge(complete_data,param_guess,how='left')\n",
    "complete_data = pd.merge(complete_data,param2_guess,how='left')\n",
    "complete_data = pd.merge(complete_data,imagesdata,how='left')\n",
    "complete_data = pd.merge(complete_data,imagesdata_pred,how='left')\n",
    "complete_data = pd.merge(complete_data,user_agg,how='left',on='user_id')\n",
    "del guess_image, price_guess, param_guess, param2_guess, imagesdata, imagesdata_pred, user_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load word2vec dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.Word2Vec.load(DATA_PATH+'avito300_sg.w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data['missing_param_1'] = complete_data['param_1'].isnull().astype(int)\n",
    "complete_data['missing_param_2'] = complete_data['param_2'].isnull().astype(int)\n",
    "complete_data['missing_param_3'] = complete_data['param_3'].isnull().astype(int)\n",
    "complete_data['missing_desc'] = complete_data['description'].isnull().astype(int)\n",
    "complete_data['missing_price'] = complete_data['price'].isnull().astype(int)\n",
    "complete_data['missing_image'] = complete_data['image'].isnull().astype(int)\n",
    "complete_data['number_missings'] = complete_data['missing_param_1']+complete_data['missing_param_2']+complete_data['missing_image']+\\\n",
    "                                    complete_data['missing_param_3']+complete_data['missing_desc']+complete_data['missing_price'] \n",
    "del complete_data['missing_param_1'] \n",
    "del complete_data['missing_param_2'] \n",
    "del complete_data['missing_param_3']\n",
    "del complete_data['missing_desc'] \n",
    "del complete_data['missing_price'] \n",
    "del complete_data['missing_image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = complete_data.groupby(['user_id'],as_index=False).agg({'item_id':'count'}).rename(columns={'item_id':'count_item'})\n",
    "big_users = set(temp[temp['count_item']>=20]['user_id'])\n",
    "complete_data['user_id'] = np.where(complete_data['user_id'].isin(big_users),complete_data['user_id'],'SmallUser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data['log_price']=np.log(complete_data['price']+0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data['city_counts']=complete_data.groupby(['city'])['item_id'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data['log_price'].fillna(-1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data['price_as_char']=complete_data['price']\n",
    "complete_data['price_as_char']=complete_data['price_as_char'].fillna(-999).astype(int).astype(str)\n",
    "tokenizer_price = keras.preprocessing.text.Tokenizer(lower=True,char_level=True)\n",
    "tokenizer_price.fit_on_texts(complete_data['price_as_char'])\n",
    "X_price = tokenizer_price.texts_to_sequences(complete_data['price_as_char'])\n",
    "X_price = keras.preprocessing.sequence.pad_sequences(X_price,padding='pre',truncating='pre',maxlen=10)\n",
    "pricechar_len = X_price.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data['log_item_seq']=np.log(0.01+complete_data['item_seq_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data['param_1'].fillna('Unknown',inplace=True)\n",
    "complete_data['param_2'].fillna('Unknown',inplace=True)\n",
    "complete_data['param_3'].fillna('Unknown',inplace=True)\n",
    "complete_data['image_top_1'].fillna('Unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = train.groupby(['city'],as_index=False).agg({'item_id':'count'}).rename(columns={'item_id':'count_item'})\n",
    "big_cities = set(temp[temp['count_item']>=min_class_cat]['city'])\n",
    "complete_data['city_clean'] = np.where(complete_data['city'].isin(big_cities),complete_data['city'],'SmallCity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "uppercase_regex = re.compile(r'[A-ZА-Я]')\n",
    "symbols_regex = re.compile(r'[^a-zA-ZА-Я0-9а-я]')\n",
    "digits_regex = re.compile(r'[0-9]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data['title_number_uppercase'] = complete_data['title'].str.count(uppercase_regex)\n",
    "complete_data['title_number_symbols'] = complete_data['title'].str.count(symbols_regex)\n",
    "complete_data['title_number_digits'] = complete_data['title'].str.count(digits_regex)\n",
    "complete_data['title_len_chars'] = complete_data['title'].apply(lambda x: len(str(x)))\n",
    "complete_data['title_len_words'] = complete_data['title'].str.split().apply(lambda x: len(str(x)))\n",
    "complete_data['title_unique'] = complete_data['title'].str.split().apply(lambda x: len(set(str(x))))\n",
    "complete_data['share_unique_title'] = complete_data['title_unique']/complete_data['title_len_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data['desc_number_uppercase'] = complete_data['description'].str.count(uppercase_regex)\n",
    "complete_data['desc_number_symbols'] = complete_data['description'].str.count(symbols_regex)\n",
    "complete_data['desc_number_digits'] = complete_data['description'].str.count(digits_regex) #should do similar stuff for param\n",
    "complete_data['desc_len_char']=complete_data['description'].apply(lambda x: len(str(x)))\n",
    "complete_data['desc_len_words']=complete_data['description'].str.split().apply(lambda x: len(str(x)))\n",
    "complete_data['desc_unique'] = complete_data['description'].str.split().apply(lambda x: len(set(str(x))))\n",
    "complete_data['share_unique_desc'] = complete_data['desc_unique']/complete_data['desc_len_words']\n",
    "complete_data['desc_rows'] = complete_data['description'].astype(str).apply(lambda x: x.count('/\\n'))\n",
    "complete_data['r_title_desc'] = complete_data['title_len_chars']/(complete_data['desc_len_char']+1)\n",
    "complete_data['desc_number_uppercase'].fillna(-1,inplace=True)\n",
    "complete_data['desc_number_symbols'].fillna(-1,inplace=True)\n",
    "complete_data['desc_number_digits'].fillna(-1,inplace=True)\n",
    "complete_data['desc_len_char'].fillna(-1,inplace=True)\n",
    "complete_data['desc_len_words'].fillna(-1,inplace=True)\n",
    "complete_data['desc_unique'].fillna(-1,inplace=True)\n",
    "complete_data['share_unique_desc'].fillna(-1,inplace=True)\n",
    "complete_data['desc_rows'].fillna(-1,inplace=True)\n",
    "complete_data['r_title_desc'].fillna(-1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in ['img_size', 'lightness','darkness','pixel_width','avg_red','avg_green','avg_blue','width','height','blurness']:\n",
    "    complete_data[var].fillna(-1,inplace=True)\n",
    "for var in ['usermean_days_up_sum','usermean_days_up_count','usermean_days_up_avg','usermean_days_until_activation_sum','usermean_days_until_activation_avg',\n",
    "            'userstd_days_up_sum','userstd_days_up_count','userstd_days_up_avg','userstd_days_until_activation_sum','userstd_days_until_activation_avg','usermedian_days_up_sum',\n",
    "            'usermedian_days_up_count','usermedian_days_up_avg','usermedian_days_until_activation_sum','usermedian_days_until_activation_avg','n_user_items']:\n",
    "    complete_data[var].fillna(-1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = set(stopwords.words('english'))\n",
    "russian_stopwords = set(stopwords.words('russian'))\n",
    "russian_stopwords = russian_stopwords.union(english_stopwords)\n",
    "def preprocess(x,stop_words=None):\n",
    "    x = keras.preprocessing.text.text_to_word_sequence(x)\n",
    "    if stop_words:\n",
    "        return [word for word in x if word not in russian_stopwords]\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_token_matrix(data,text_col,num_words,max_len,stop_words=None):\n",
    "    \n",
    "    print('Create Tokenizer...',end=' ')\n",
    "    \n",
    "    texts = data[text_col].astype(str)\n",
    "    tokenizer = keras.preprocessing.text.Tokenizer(num_words=num_words,lower=True)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    \n",
    "    print('Preprocess Text...',end=' ')\n",
    "    texts = texts.apply(lambda x: preprocess(x,stop_words))\n",
    "    \n",
    "    print('Create Matrix...',end=' ')\n",
    "    X = tokenizer.texts_to_sequences(texts)\n",
    "    X = keras.preprocessing.sequence.pad_sequences(X,padding='pre',truncating='post',maxlen=max_len)\n",
    "    \n",
    "    print('Done !')\n",
    "    return X,tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data['description'] = complete_data['description'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Tokenizer... Preprocess Text... Create Matrix... Done !\n"
     ]
    }
   ],
   "source": [
    "X_desc, tokenizer_desc = make_token_matrix(complete_data,'description',num_words,max_len,stop_words=russian_stopwords)\n",
    "word2idx = tokenizer_desc.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = tokenizer_desc.word_index\n",
    "idx2word = {i:w for w,i in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "кокон сна малыша пользовались меньше месяца цвет серый "
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Кокон для сна малыша,пользовались меньше месяца.цвет серый'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in X_desc[0]:\n",
    "    if i!=0:\n",
    "        print(idx2word[i],end=' ')\n",
    "complete_data.iloc[0]['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Tokenizer... Preprocess Text... Create Matrix... Done !\n"
     ]
    }
   ],
   "source": [
    "X_title, tokenizer_title = make_token_matrix(complete_data,'title',num_words,max_len,stop_words=russian_stopwords)    \n",
    "word2idx_title = tokenizer_title.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data['labels_all'] = (complete_data['Res50_label1'].astype(str)+' '+complete_data['Xcept_label1'].astype(str)+' '+complete_data['Incept_label1'].astype(str)+' '+\n",
    "                               complete_data['Res50_label2'].astype(str)+' '+complete_data['Xcept_label2'].astype(str)+' '+complete_data['Incept_label2'].astype(str)+' '+\n",
    "                               complete_data['Res50_label3'].astype(str)+' '+complete_data['Xcept_label3'].astype(str)+' '+complete_data['Incept_label3'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Tokenizer... Preprocess Text... Create Matrix... Done !\n"
     ]
    }
   ],
   "source": [
    "X_labels, tokenizer_labels = make_token_matrix(complete_data,'labels_all',num_words,9)    \n",
    "word2idx_labels = tokenizer_labels.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = ['param_1','image_top_1','city_clean','user_type','category_name']\n",
    "categoricals += ['param_2','param_3']\n",
    "#categoricals += ['user_id']\n",
    "\n",
    "numericals = ['log_price','log_item_seq','number_missings']\n",
    "#numericals += ['title_number_uppercase','title_number_symbols','title_number_digits','title_len_chars','title_len_words','title_unique','share_unique_title']\n",
    "#numericals += ['desc_number_uppercase','desc_number_symbols','desc_number_digits','desc_len_char','desc_len_words','desc_unique','share_unique_desc','desc_rows','r_title_desc']\n",
    "#numericals += [ 'lightness','darkness','avg_red','avg_green','avg_blue','width','height','blurness']\n",
    "numericals += ['usermean_days_up_sum','n_user_items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "catembs = []\n",
    "le = LabelEncoder()\n",
    "for cat in categoricals: #Must deal with cities not in train set\n",
    "    complete_data[cat].fillna(-999,inplace=True)\n",
    "    complete_data[cat] = le.fit_transform(complete_data[cat].astype(str))\n",
    "    catembs.append(complete_data[cat].max()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat = complete_data[categoricals].values.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[372, 3064, 674, 3, 47, 9, 278, 1277, 3890]\n"
     ]
    }
   ],
   "source": [
    "print(catembs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dense = scaler.fit_transform(complete_data[numericals]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "dense_len = X_dense.shape[1]\n",
    "print(dense_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate and Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = [0, \n",
    "          max_len, #Description\n",
    "          max_len+max_len, #Title\n",
    "          max_len+max_len+dense_len, #Dense\n",
    "          *[max_len+max_len+dense_len+i+1 for i in range(len(catembs))]] #Categoricals as embeding layer \n",
    "\n",
    "slices_bounds = [(slices[i],slices[i+1]) for i,s in enumerate(slices) if i<len(slices)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([X_desc,X_title,X_dense,X_cat],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = X[:len(train),:].astype('float32')\n",
    "X_board = X[len(train):,:].astype('float32')\n",
    "\n",
    "y = train['deal_probability']\n",
    "del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 150),\n",
       " (150, 300),\n",
       " (300, 306),\n",
       " (306, 307),\n",
       " (307, 308),\n",
       " (308, 309),\n",
       " (309, 310),\n",
       " (310, 311),\n",
       " (311, 312),\n",
       " (312, 313),\n",
       " (313, 314),\n",
       " (314, 315)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slices_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretrain_embedding(w2idx,word2vec,embed_dim,num_words):\n",
    "    unknown_words = []\n",
    "    embeddings = np.zeros((num_words+1,embed_dim))  #0 is a special token\n",
    "    for word,idx in w2idx.items(): #starts at 1    \n",
    "        if idx>num_words:\n",
    "            break \n",
    "        try:\n",
    "            vect = word2vec[word]\n",
    "            embeddings[idx]=vect/np.linalg.norm(vect)\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    print('Number of words with no embeddings',len(unknown_words))\n",
    "    \n",
    "    return embeddings, unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with no embeddings 210\n",
      "Number of words with no embeddings 7831\n"
     ]
    }
   ],
   "source": [
    "pretrained_desc, unknown_words1 = make_pretrain_embedding(word2idx,word2vec,300,num_words)\n",
    "pretrained_title, unknown_words2 = make_pretrain_embedding(word2idx_title,word2vec,300,num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nn_model(desc_len,title_len,dense_len,catembs):\n",
    "    \n",
    "    desc_input = keras.layers.Input(shape=(desc_len,))\n",
    "    title_input = keras.layers.Input(shape=(title_len,))\n",
    "    dense_input = keras.layers.Input(shape=(dense_len,))\n",
    "    \n",
    "    desc_embedded = keras.layers.Embedding(input_dim=pretrained_desc.shape[0],\n",
    "                                  output_dim=300,\n",
    "                                  input_length=desc_len,\n",
    "                                  weights=[pretrained_desc],\n",
    "                                  trainable=False)(desc_input)\n",
    "    desc_features = keras.layers.Bidirectional(keras.layers.CuDNNGRU(64,return_sequences = True))(desc_embedded)\n",
    "    desc_features1 = keras.layers.GlobalAveragePooling1D()(desc_features)\n",
    "    desc_features2 = keras.layers.GlobalMaxPooling1D()(desc_features)\n",
    "    desc_features = keras.layers.Concatenate()([desc_features1,desc_features2])\n",
    "    \n",
    "    \n",
    "    title_embedded = keras.layers.Embedding(input_dim=pretrained_title.shape[0],\n",
    "                                  output_dim=300,\n",
    "                                  input_length=title_len,\n",
    "                                  weights=[pretrained_title],\n",
    "                                  trainable=False)(title_input)\n",
    "    title_features = keras.layers.Bidirectional(keras.layers.CuDNNGRU(64,return_sequences = True))(title_embedded)\n",
    "    title_features1 = keras.layers.GlobalAveragePooling1D()(title_features)\n",
    "    title_features2 = keras.layers.GlobalMaxPooling1D()(title_features)\n",
    "    title_features = keras.layers.Concatenate()([title_features1,title_features2])\n",
    "    \n",
    "    \n",
    "    \n",
    "    #############################################\n",
    "    labels_input = keras.layers.Input(shape=(9,))\n",
    "    labels_embedded = keras.layers.Embedding(input_dim=1050,\n",
    "                                  output_dim=64,\n",
    "                                  input_length=9,\n",
    "                                  trainable=True)(labels_input)\n",
    "    labels_features = keras.layers.CuDNNGRU(32,return_sequences = False)(labels_embedded)\n",
    "    #############################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    cat_embs_inputs = []\n",
    "    cat_embs_embeded = []\n",
    "    for i in range(len(catembs)):\n",
    "        inp = keras.layers.Input(shape=(1,))\n",
    "        cat_embs_inputs.append(inp)\n",
    "        embed = keras.layers.Embedding(input_dim=catembs[i],output_dim=min(64,catembs[i]//2),input_length=1,trainable=True)(inp)\n",
    "        embed = keras.layers.Dropout(0.3)(embed)\n",
    "        embed = keras.layers.Flatten()(embed)\n",
    "        cat_embs_embeded.append(embed)\n",
    "        \n",
    "    dense_features = keras.layers.Concatenate()(cat_embs_embeded+[dense_input])\n",
    "    dense_features = keras.layers.Dense(512,kernel_initializer='he_normal')(dense_features)\n",
    "    dense_features = keras.layers.PReLU()(dense_features)\n",
    "    dense_features = keras.layers.BatchNormalization()(dense_features)\n",
    "    dense_features = keras.layers.Dropout(0.3)(dense_features)\n",
    "    dense_features = keras.layers.Concatenate()([dense_features,title_features,desc_features])\n",
    "    dense_features = keras.layers.Dense(512,kernel_initializer='he_normal')(dense_features)\n",
    "    dense_features = keras.layers.PReLU()(dense_features)\n",
    "    dense_features = keras.layers.BatchNormalization()(dense_features)\n",
    "    dense_features = keras.layers.Dropout(0.3)(dense_features)\n",
    "    dense_features = keras.layers.Dense(128,kernel_initializer='he_normal')(dense_features)\n",
    "    dense_features = keras.layers.PReLU()(dense_features)\n",
    "    dense_features = keras.layers.BatchNormalization()(dense_features)\n",
    "    dense_features = keras.layers.Dropout(0.3)(dense_features)\n",
    "    dense_features = keras.layers.Dense(64,kernel_initializer='he_normal')(dense_features)\n",
    "    dense_features = keras.layers.PReLU()(dense_features)\n",
    "    dense_features = keras.layers.BatchNormalization()(dense_features)\n",
    "    dense_features = keras.layers.Dropout(0.3)(dense_features)\n",
    "    dense_features = keras.layers.Dense(64,kernel_initializer='he_normal')(dense_features)\n",
    "    dense_features = keras.layers.PReLU()(dense_features)\n",
    "    dense_features = keras.layers.BatchNormalization()(dense_features)\n",
    "    dense_features = keras.layers.Dropout(0.3)(dense_features)\n",
    "    dense_features = keras.layers.Dense(32,kernel_initializer='he_normal')(dense_features)\n",
    "    dense_features = keras.layers.PReLU()(dense_features)\n",
    "    dense_features = keras.layers.BatchNormalization()(dense_features)\n",
    "    dense_features = keras.layers.Dropout(0.3)(dense_features)\n",
    "    dense_features = keras.layers.Dense(32,kernel_initializer='he_normal')(dense_features)\n",
    "    dense_features = keras.layers.PReLU()(dense_features)\n",
    "    dense_features = keras.layers.BatchNormalization()(dense_features)\n",
    "            \n",
    "    y_hat = keras.layers.Dense(1,activation='sigmoid')(dense_features)\n",
    "    \n",
    "    nn_model = keras.Model(inputs=[desc_input,title_input,dense_input,*cat_embs_inputs, labels_input],outputs=y_hat)\n",
    "    \n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd2dda29778464f930e9cdad717a12c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f98301b77ff4548aaee7cd352f518b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=1353081), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e14d5f58847457db0c5a6391231f1fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=1353081), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2143cee0d16642b2963d59c35b566f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=1353081), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5315ab35deb45069285738c7355de5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 3', max=1353081), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44986c1e3703426491f65056b1e45935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 4', max=1353081), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5afef0808024cb1b5719d9fd99d59e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 5', max=1353081), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51bfcc4e604b4fd9a0589d6d03802c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 6', max=1353081), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139fe03a9ff948af96cc750076ac9747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 7', max=1353081), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3858048e5a894ab3954e6799bb8db7e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 8', max=1353081), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b31ccbc3ad646398a6816f150c2eb42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 9', max=1353081), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 0.2189\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "np.random.seed(seed)\n",
    "\n",
    "X_lab_tr = X_labels[:len(train),:].astype('float32')\n",
    "\n",
    "#X_tr_tr, X_val, y_tr, y_val = train_test_split(X_tr,y,test_size=0.1,random_state=seed)\n",
    "X_tr_tr, X_val, X_labels_tr, X_labels_val, y_tr, y_val = train_test_split(X_tr,X_lab_tr,y,test_size=0.1,random_state=seed)\n",
    "\n",
    "keras.backend.clear_session() #Reset   \n",
    "nn_model = make_nn_model(max_len,max_len,dense_len,catembs)\n",
    "Adam = keras.optimizers.Adam(0.001)\n",
    "nn_model.compile(optimizer=Adam,loss='mean_squared_error',metrics=[rmse])\n",
    "epochs = 10 #Beware of overfit\n",
    "nn_model.fit([X_tr_tr[:,s[0]:s[1]] for s in slices_bounds]+[X_labels_tr], y_tr,\n",
    "          validation_data=([X_val[:,s[0]:s[1]] for s in slices_bounds]+[X_labels_val], y_val),\n",
    "          batch_size=1024,epochs=epochs,verbose = 0,callbacks=[TQDMNotebookCallback(leave_inner=True)])\n",
    "predict_val = nn_model.predict([X_val[:,s[0]:s[1]] for s in slices_bounds]+[X_labels_val]).flatten().clip(0.0,1.0)\n",
    "r = mean_squared_error(y_val,predict_val)**0.5\n",
    "print(f'RMSE : {r:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cnn_model.predict([X_board[:,s[0]:s[1]] for s in slices_bounds])\n",
    "test['deal_probability']=predictions.clip(0.0,1.0)\n",
    "test[['item_id','deal_probability']].to_csv(DATA_PATH+f'{r:.5f}_'+'Predictions_NNv2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
